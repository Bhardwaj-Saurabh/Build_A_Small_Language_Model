{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33e6c330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a9ab93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple implementation of the Self-Attention mechanism.\n",
    "\n",
    "    Self-Attention allows each element in a sequence to attend (i.e., \n",
    "    look at and weigh) all other elements in the sequence when forming \n",
    "    a new representation. This is the core mechanism behind \n",
    "    Transformer models.\n",
    "\n",
    "    Args:\n",
    "        d_in (int): Dimensionality of the input features.\n",
    "        d_out (int): Dimensionality of the output (query/key/value vectors).\n",
    "\n",
    "    Forward Input:\n",
    "        x (torch.Tensor): Input tensor of shape (N, d_in), where\n",
    "            - N is the number of tokens (sequence length).\n",
    "            - d_in is the feature size of each token.\n",
    "\n",
    "    Forward Output:\n",
    "        context_vec (torch.Tensor): Tensor of shape (N, d_out),\n",
    "            where each output token representation is a weighted sum of\n",
    "            all input token value vectors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out):\n",
    "        # Initialize the parent nn.Module class\n",
    "        super().__init__()\n",
    "        \n",
    "        # Linear projection to generate query vectors (Q)\n",
    "        # Each input vector (d_in) is mapped to query space (d_out)\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        # Linear projection to generate key vectors (K)\n",
    "        # Each input vector (d_in) is mapped to key space (d_out)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=False)\n",
    "\n",
    "        # Linear projection to generate value vectors (V)\n",
    "        # Each input vector (d_in) is mapped to value space (d_out)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of self-attention.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor of shape (N, d_in).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output context vectors of shape (N, d_out).\n",
    "        \"\"\"\n",
    "\n",
    "        # Project the input into query space\n",
    "        queries = self.W_query(x)   # Shape: (N, d_out)\n",
    "\n",
    "        # Project the input into key space\n",
    "        keys = self.W_key(x)        # Shape: (N, d_out)\n",
    "\n",
    "        # Project the input into value space\n",
    "        values = self.W_value(x)    # Shape: (N, d_out)\n",
    "        \n",
    "        # Compute attention scores by taking dot product between queries and keys\n",
    "        # queries: (N, d_out), keys.T: (d_out, N) => att_scores: (N, N)\n",
    "        att_scores = queries @ keys.T \n",
    "\n",
    "        # Scale the scores by sqrt(d_out) to stabilize gradients\n",
    "        att_scores = att_scores / keys.shape[-1]**0.5\n",
    "\n",
    "        # Apply softmax to normalize the scores into attention weights\n",
    "        # Each row sums to 1, representing the distribution over all tokens\n",
    "        att_weights = torch.softmax(att_scores, dim=-1)  # Shape: (N, N)\n",
    "        \n",
    "        # Compute the weighted sum of values using attention weights\n",
    "        # att_weights: (N, N), values: (N, d_out) => context_vec: (N, d_out)\n",
    "        context_vec = att_weights @ values\n",
    "\n",
    "        # Return the new representation for each token\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Causal (autoregressive) self-attention.\n",
    "\n",
    "    This layer computes standard self-attention but applies a *causal mask* so\n",
    "    that token t can only attend to positions <= t (no peeking into the future).\n",
    "    This is the attention used in decoder-only Transformers (e.g., GPT).\n",
    "\n",
    "    Args:\n",
    "        d_in (int): Input embedding dimensionality.\n",
    "        d_out (int): Projection dimensionality for queries/keys/values and outputs.\n",
    "        context_length (int): Maximum sequence length supported by the mask.\n",
    "        dropout (float, optional): Dropout probability applied to attention weights.\n",
    "        qkv_bias (bool, optional): Whether to include bias in Q/K/V linear layers.\n",
    "\n",
    "    Shapes:\n",
    "        Input:  x of shape (B, T, d_in)\n",
    "                B = batch size, T = sequence length\n",
    "        Output: context_vec of shape (B, T, d_out)\n",
    "\n",
    "    Notes:\n",
    "        - The causal mask is registered as a non-trainable buffer so it moves with\n",
    "          the module across devices and is saved in state_dict.\n",
    "        - Attention scores are scaled by sqrt(d_out) for stable gradients.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, context_length, \n",
    "                 dropout=0.0, qkv_bias=False):\n",
    "        super().__init__()  # Initialize base nn.Module state\n",
    "\n",
    "        # Linear projections to form Q, K, V from inputs.\n",
    "        # Each maps d_in -> d_out; optional bias controlled by qkv_bias.\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "\n",
    "        # Dropout used *after* softmax on attention weights.\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Upper-triangular matrix with ones above the main diagonal:\n",
    "        # mask[i, j] = 1 if j > i (future positions), else 0.\n",
    "        # Registered as a buffer so it's not a parameter, but moves with the module.\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length), diagonal=1)  # (context_length, context_length)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (torch.Tensor): (B, T, d_in)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (B, T, d_out) context vectors.\n",
    "        \"\"\"\n",
    "        # Unpack shapes: B=batch, T=sequence length, C=input channels (d_in).\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # Project inputs to queries/keys/values.\n",
    "        # Shapes: (B, T, d_out) for each.\n",
    "        queries = self.W_query(x)\n",
    "        keys    = self.W_key(x)\n",
    "        values  = self.W_value(x)\n",
    "        \n",
    "        # Raw attention scores via batched matrix multiply:\n",
    "        # queries: (B, T, d_out), keys.transpose(1, 2): (B, d_out, T)\n",
    "        # => att_scores: (B, T, T) (each token attends over all tokens)\n",
    "        att_scores = queries @ keys.transpose(1, 2)\n",
    "\n",
    "        # Apply the causal mask: disallow attention to future positions.\n",
    "        # self.mask[:T, :T] is (T, T); broadcast across batch to (B, T, T).\n",
    "        # Positions where mask==1 (future) are set to -inf so softmax -> 0.\n",
    "        att_scores.masked_fill_(self.mask.bool()[:T, :T], -torch.inf)\n",
    "\n",
    "        # Scale by sqrt(d_out) to stabilize gradients/magnitudes.\n",
    "        # (see note below about using the correct dimension)\n",
    "        att_scores = att_scores / (queries.shape[-1] ** 0.5)\n",
    "\n",
    "        # Normalize scores along the key dimension to obtain attention weights.\n",
    "        # (B, T, T), each row sums to 1.\n",
    "        att_weights = torch.softmax(att_scores, dim=-1)\n",
    "\n",
    "        # Regularize by dropping some attention probability mass.\n",
    "        att_weights = self.dropout(att_weights)\n",
    "        \n",
    "        # Weighted average of values: (B, T, T) @ (B, T, d_out) -> (B, T, d_out)\n",
    "        context_vec = att_weights @ values\n",
    "\n",
    "        # Return per-token context-enhanced representations.\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d7b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionWrapper(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple wrapper that composes multiple independent causal-attention heads\n",
    "    and concatenates their outputs along the feature dimension.\n",
    "\n",
    "    Each head is an instance of `CausalAttention`, so the masking behavior is\n",
    "    autoregressive (token t cannot attend to positions > t).\n",
    "\n",
    "    Args:\n",
    "        d_in (int): Input embedding size for each token.\n",
    "        d_out (int): Output size *per head* for each CausalAttention module.\n",
    "        num_heads (int): Number of attention heads to run in parallel.\n",
    "        context_length (int): Max sequence length supported by each headâ€™s mask.\n",
    "        dropout (float, optional): Dropout probability on attention weights.\n",
    "        qkv_bias (bool, optional): Whether Q/K/V linear layers include bias.\n",
    "\n",
    "    Shapes:\n",
    "        Input:\n",
    "            x: (B, T, d_in)\n",
    "        Output:\n",
    "            y: (B, T, num_heads * d_out)\n",
    "\n",
    "        where:\n",
    "            B = batch size, T = sequence length.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_in, d_out, \n",
    "                 num_heads, context_length, \n",
    "                 dropout=0.0, qkv_bias=False):\n",
    "        # Initialize base nn.Module\n",
    "        super().__init__()\n",
    "\n",
    "        # Create 'num_heads' independent causal attention heads.\n",
    "        # Each head maps (B, T, d_in) -> (B, T, d_out).\n",
    "        # Using ModuleList keeps heads as registered submodules.\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                CausalAttention(\n",
    "                    d_in=d_in,\n",
    "                    d_out=d_out,\n",
    "                    context_length=context_length,\n",
    "                    dropout=dropout,\n",
    "                    qkv_bias=qkv_bias\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Runs all heads on the same input and concatenates their outputs.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): (B, T, d_in)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: (B, T, num_heads * d_out)\n",
    "        \"\"\"\n",
    "        # Compute each head's output: list of (B, T, d_out)\n",
    "        head_outputs = [h(x) for h in self.heads]\n",
    "\n",
    "        # Concatenate along the channel (feature) dimension: (B, T, num_heads*d_out)\n",
    "        return torch.cat(head_outputs, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a3c7dd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "build-a-small-language-model",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
