{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e31d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import tiktoken\n",
    "import math\n",
    "from datasets import load_dataset\n",
    "from datasets import load_from_disk\n",
    "from dataclasses import dataclass\n",
    "from contextlib import nullcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05164c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TinyStories dataset from the Hugging Face Hub\n",
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "display(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "749d8ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (4/4 shards): 100%|██████████| 2119719/2119719 [00:01<00:00, 1697668.16 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 21990/21990 [00:00<00:00, 1390701.96 examples/s]\n"
     ]
    }
   ],
   "source": [
    " # Preprocess and save the dataset to disk\n",
    " dataset.save_to_disk(\"../tinystories_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbb9b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2119719\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 21990\n",
       "    })\n",
       "})"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To load the dataset from disk in future sessions, use:\n",
    "ds = load_from_disk(\"../tinystories_dataset\")\n",
    "display(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fdaa826d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the first example from the training split\n",
    "example = ds['train'][0]\n",
    "\n",
    "# Display the example\n",
    "display(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d4ee2661",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(example):\n",
    "    \"\"\"Tokenize a single example into integer IDs and report its length.\n",
    "\n",
    "    This function expects `example` to be a mapping (e.g., a dict) with a\n",
    "    'text' field containing a string. It uses a global/tokenizer object\n",
    "    `enc` with a method `encode_ordinary` to convert the text into a list/array\n",
    "    of token IDs (without adding special tokens). It returns a dict with:\n",
    "\n",
    "        - 'ids': the tokenized sequence (list/array of ints)\n",
    "        - 'len': the number of tokens (int)\n",
    "\n",
    "    Args:\n",
    "        example (Mapping[str, Any]): An item from the dataset containing\n",
    "            at least the key 'text' mapped to a string.\n",
    "\n",
    "    Returns:\n",
    "        dict: {'ids': <token_ids>, 'len': <length_of_token_ids>}\n",
    "    \"\"\"\n",
    "    # Convert raw text into a sequence of token IDs using the tokenizer.\n",
    "    # `encode_ordinary` typically means: no BOS/EOS or special tokens inserted.\n",
    "    ids = enc.encode_ordinary(example['text'])\n",
    "\n",
    "    # Package both the token IDs and their length for downstream batching/writing.\n",
    "    out = {'ids': ids, 'len': len(ids)}\n",
    "\n",
    "    # Return the tokenization result for this example.\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b8aa2f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizing the splits (num_proc=8): 100%|██████████| 2119719/2119719 [00:36<00:00, 57780.92 examples/s]\n",
      "tokenizing the splits (num_proc=8): 100%|██████████| 21990/21990 [00:00<00:00, 34057.11 examples/s]\n",
      "writing ../preprocess_data/train.bin: 100%|██████████| 1024/1024 [04:36<00:00,  3.71it/s]\n",
      "writing ../preprocess_data/validation.bin: 100%|██████████| 1024/1024 [00:03<00:00, 260.07it/s]\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------------------------------------------------------\n",
    "# Dataset preprocessing to create binary memmap files per split (e.g., train.bin).\n",
    "#\n",
    "# This script:\n",
    "#   1) Checks if \"train.bin\" already exists to avoid reprocessing.\n",
    "#   2) Maps a tokenization function `process` over the dataset `ds`\n",
    "#      (removing raw 'text' and running in parallel with num_proc=8).\n",
    "#   3) For each split in the tokenized dataset, preallocates a memory-mapped\n",
    "#      array sized to the total token count (sum of 'len').\n",
    "#   4) Writes tokens to the memmap in contiguous shards using dataset sharding.\n",
    "#\n",
    "# Requirements / globals (expected to be defined elsewhere):\n",
    "#   - ds: a datasets.DatasetDict or similar with text and metadata fields.\n",
    "#   - process: a callable that tokenizes each example and returns fields\n",
    "#              including 'ids' (token IDs, numpy arrays) and 'len' (lengths).\n",
    "#   - np: NumPy\n",
    "#   - tqdm: progress bar utility (from tqdm import tqdm)\n",
    "#   - os: standard library os module\n",
    "#\n",
    "# Output:\n",
    "#   - For each split (e.g., 'train', 'validation'): creates \"<split>.bin\"\n",
    "#     containing the concatenated token IDs as a uint64 memmap.\n",
    "#\n",
    "# Notes:\n",
    "#   - dtype is set to np.uint64 here; ensure downstream readers (e.g., get_batch)\n",
    "#     use the same dtype, or perform explicit casting when loading.\n",
    "#   - with_format('numpy') ensures batches yield NumPy arrays for fast concat.\n",
    "#   - Sharding with contiguous=True preserves order within each shard.\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "if not os.path.exists(\"../preprocess_data/train.bin\"):\n",
    "    tokenized = ds.map(\n",
    "        process,\n",
    "        remove_columns = ['text'],\n",
    "        desc = \"tokenizing the splits\",\n",
    "        num_proc = 8,\n",
    "    )\n",
    "\n",
    "    for split, dset in tokenized.items():\n",
    "        # Total number of tokens across the split; used to size the memmap.\n",
    "        arr_len = np.sum(dset['len'], dtype=np.uint64)\n",
    "\n",
    "        # Output filename for the current split.\n",
    "        filename = f'../preprocess_data/{split}.bin'\n",
    "\n",
    "        # Binary dtype for stored token IDs; keep consistent across pipeline.\n",
    "        dtype = np.uint64\n",
    "\n",
    "        # Preallocate a writable memmap of the exact total length.\n",
    "        arr = np.memmap(filename, dtype=dtype, mode='w+', shape=(arr_len))\n",
    "\n",
    "        # Number of shards to iterate over when writing to memmap.\n",
    "        total_batches = 1024\n",
    "\n",
    "        # Current write position within the memmap.\n",
    "        idx = 0\n",
    "\n",
    "        # Iterate over shards/batches and write them sequentially into the memmap.\n",
    "        for batch_idx in tqdm(range(total_batches), desc=f'writing {filename}'):\n",
    "            # Take a contiguous shard of the dataset and return NumPy arrays.\n",
    "            batch = dset.shard(num_shards=total_batches, index=batch_idx, contiguous=True).with_format('numpy')\n",
    "\n",
    "            # Concatenate the token ID arrays from the shard into one array.\n",
    "            arr_batch = np.concatenate(batch['ids'])\n",
    "\n",
    "            # Write the batch into the memmap slice and advance the write index.\n",
    "            arr[idx: idx + len(arr_batch)] = arr_batch\n",
    "            idx += len(arr_batch)\n",
    "\n",
    "        # Ensure all buffered writes are flushed to disk.\n",
    "        arr.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b819dd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f880b38d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
